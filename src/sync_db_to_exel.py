import pandas as pd
import pymysql
import os
from sqlalchemy import create_engine
import json

# å¼‚æ­¥æ“ä½œè„šæœ¬ï¼Œä¸åœ¨main.pyå†…ï¼Œ
# é—¨åº—åŸºç¡€è¡¨ï¼Œå–ä¸Šæœˆæœ€åä¸€å¤©çš„é—¨åº—è¡¨æ¥åšé—¨åº—åŸºç¡€è¡¨ï¼Œ
# ä»æ•°æ®åº“åŠ è½½åˆ°æœ¬åœ°excelï¼Œæé«˜å‰ç«¯å“åº”é€Ÿåº¦

# --- Database Configuration ---
DB_CONFIG = {
    "host": "10.243.0.221",
    "port": 3306,
    "user": "xinpin",
    "password": "xinpin",
    "database": "new_goods_manage"
}

# --- SQL Query ---
SQL_QUERY = """
SELECT  shop_code                    AS `é—¨åº—sapid`
       ,lev3_org_name                AS `DHRæˆ˜åŒº`
       ,lev3_org_name_xp             AS `ææŠ¥æˆ˜åŒº`
       ,sales_scan_name              AS `é”€å”®è§„æ¨¡`
       ,forbid_goods_aprl_types_code AS `å—é™æ‰¹æ–‡åˆ†ç±»ç¼–ç `
       ,forbid_goods_aprl_types_name AS `å—é™æ‰¹æ–‡åˆ†ç±»åç§°`
       ,shop_update_time             AS `é—¨åº—è¡¨æ›´æ–°æ—¶é—´`
       ,company_name                 AS `çœå…¬å¸`
       ,city_name                    AS `åŸå¸‚`
       ,prov_name                    AS `çœä»½`
       ,shop_age_and_type_name       AS `åº—é¾„åº—å‹`
       ,busi_district_type_name      AS `å®¢æµå•†åœˆ`
       ,admin_area_name              AS `è¡Œæ”¿åŒºåˆ’ç­‰çº§`
       ,shop_o2o_type                AS `å…¬åŸŸO2Oåº—å‹`
       ,is_focus_shop_o2o            AS `æ˜¯å¦O2Oé—¨åº—`
       ,is_med_insu_shop             AS `æ˜¯å¦åŒ»ä¿åº—`
       ,is_op_coor_shop              AS `æ˜¯å¦ç»Ÿç­¹åº—`
FROM xp_dist_fee_shop_tag_dfp
WHERE dt = LAST_DAY(DATE_SUB(CURDATE(), INTERVAL 1 MONTH)) 
"""

def sync_data():
    """
    Connects to MySQL, executes the query, and saves the result to an Excel file.
    """
    print("ğŸš€ Starting database sync...")
    
    # 1. Create SQLAlchemy Engine
    connection_str = f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
    
    try:
        engine = create_engine(connection_str)
        
        # 2. Execute Query & Load into DataFrame
        print("ğŸ“¥ Fetching data from MySQL...")
        df = pd.read_sql(SQL_QUERY, engine)
        
        # 3. Data Transformation (Optional)
        row_count = len(df)
        print(f"âœ… Fetched {row_count} rows.")

        # 4. Save to Excel
        # Ensure the data directory exists
        current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        data_dir = os.path.join(current_dir, "data")
        os.makedirs(data_dir, exist_ok=True)
        
        # è¡¨1ï¼šé—¨åº—ä¿¡æ¯è¡¨
        output_path = os.path.join(data_dir, "store_master.xlsx")
        print(f"ğŸ’¾ Saving to {output_path}...")
        df.to_excel(output_path, index=False, engine='openpyxl')
        
        # 5. è¡¨2ï¼šGenerate Region Map (Unique combinations of Company/Province/City)
        region_map_path = os.path.join(data_dir, "region_map.xlsx")
        print(f"ğŸ’¾ Generating region map to {region_map_path}...")
        region_df = df[['çœå…¬å¸', 'çœä»½', 'åŸå¸‚']].dropna().drop_duplicates().sort_values(['çœå…¬å¸', 'çœä»½', 'åŸå¸‚'])
        region_df.to_excel(region_map_path, index=False, engine='openpyxl')
        
        # 6. Generate Dimension Metadata (JSON for UI dropdowns)
        metadata_path = os.path.join(data_dir, "dim_metadata.json")
        print(f"ğŸ’¾ Generating dimension metadata to {metadata_path}...")
        
        # è¡¨3ï¼šè‡ªå®šä¹‰ç­›é€‰æ¡ä»¶å­—æ®µéƒ½å­˜ä¸ºä¸€ä¸ªå…ƒæ•°æ®
        metadata = {
            "åº—é¾„åº—å‹": sorted(df["åº—é¾„åº—å‹"].dropna().unique().tolist()),
            "è¡Œæ”¿åŒºåˆ’ç­‰çº§": sorted(df["è¡Œæ”¿åŒºåˆ’ç­‰çº§"].dropna().unique().tolist()),
            "å…¬åŸŸO2Oåº—å‹": sorted(df["å…¬åŸŸO2Oåº—å‹"].dropna().unique().tolist()),
            # ç‰¹æ®Šå¤„ç†ï¼šå®¢æµå•†åœˆ (é€—å·åˆ†éš”)
            "å®¢æµå•†åœˆ": sorted(list(set([
                p.strip() 
                for val in df["å®¢æµå•†åœˆ"].dropna().astype(str) 
                for p in val.replace("ï¼Œ", ",").split(",") if p.strip()
            ]))),
            "é”€å”®è§„æ¨¡": ["è¶…çº§æ——èˆ°åº—", "æ——èˆ°åº—", "å¤§åº—", "ä¸­åº—", "å°åº—", "æˆé•¿åº—"],
            # æ–°å¢ï¼šä¸šåŠ¡å±æ€§å¸ƒå°”å€¼æå–
            "æ˜¯å¦åŒ»ä¿åº—": sorted(df["æ˜¯å¦åŒ»ä¿åº—"].dropna().unique().tolist()),
            "æ˜¯å¦O2Oé—¨åº—": sorted(df["æ˜¯å¦O2Oé—¨åº—"].dropna().unique().tolist()),
            "æ˜¯å¦ç»Ÿç­¹åº—": sorted(df["æ˜¯å¦ç»Ÿç­¹åº—"].dropna().unique().tolist()),
            "æ›´æ–°æ—¶é—´": str(df["é—¨åº—è¡¨æ›´æ–°æ—¶é—´"].iloc[0]) if "é—¨åº—è¡¨æ›´æ–°æ—¶é—´" in df.columns else "æœªçŸ¥"
        }
        
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print("ğŸ‰ Sync completed successfully!")
        
    except Exception as e:
        print(f"âŒ Error during sync: {e}")

if __name__ == "__main__":
    sync_data()
